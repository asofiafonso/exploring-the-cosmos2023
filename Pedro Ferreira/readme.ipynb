{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c6f4cc",
   "metadata": {},
   "source": [
    "# Exploring COSMOS with machine learning: galaxies’ physical properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c7a08",
   "metadata": {},
   "source": [
    "###### Pedro António Malta Ferreira"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed8fac",
   "metadata": {},
   "source": [
    "This notebook aims to concisely and comprehensibly describe the steps used throughout the study in question.\n",
    "In total, 5 Jupyter notebooks were developed, whose names are:\n",
    "\n",
    "- data_upload.ipynb\n",
    "- data_preparation.ipynb\n",
    "- ML_implementation.ipynb\n",
    "- ML_algorithm_analysis.ipynb\n",
    "- ML_algorithm_analysis_EASY.ipynb\n",
    "\n",
    "Let's analyze each one in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b929f",
   "metadata": {},
   "source": [
    "##### data_upload.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1cda0d",
   "metadata": {},
   "source": [
    "This notebook consists of establishing a first contact with the catalogs. Initially, we proceed with the opening of the .fit files where all the information we are looking for is available. A pre-selection of the data we wanted has already been done; as the study focuses on galaxies (using the 'lp_type' parameter), we removed irrelevant structures and (optionally) added a flag parameter to eliminate potential unreliable values. Subsequently, the catalogs were saved in .parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#catalog = catalog[catalog['FLAG_COMBINED'] == 0]\n",
    "catalog = catalog[catalog['lp_type'] == 0]\n",
    "catalog = catalog.to_pandas()\n",
    "catalog = catalog.to_parquet('path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de84767",
   "metadata": {},
   "source": [
    "##### data_preparation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d6c47",
   "metadata": {},
   "source": [
    "This notebook consists of the preselection of features and labels that we intend to extract from the catalogs. In this version (quickly adapted for other configurations), only magnitudes and fluxes whose aperture corresponds to APER3 were selected as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_classic = set()\n",
    "columns_farmer_not_classic = set()\n",
    "\n",
    "for column in catalog_classic.columns:\n",
    "    if 'APER3' in column:\n",
    "        if 'ERR' not in column:\n",
    "            columns_classic.add(column)\n",
    "\n",
    "for column in catalog_farmer.columns:\n",
    "    if ('MAG' in column or 'FLUX' in column):\n",
    "        if 'ERR' not in column:\n",
    "            if column not in columns_classic:\n",
    "                columns_farmer_not_classic.add(column)\n",
    "\n",
    "columns_classic = list(columns_classic)\n",
    "columns_farmer_not_classic = list(columns_farmer_not_classic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e12c60",
   "metadata": {},
   "source": [
    "Two catalogs were created by merging the Classic and Farmer datasets, one containing the feature data and the other containing the labels that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cat_classic.merge(cat_farmer,left_index=True, right_index=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = cat_labels.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0280f9f2",
   "metadata": {},
   "source": [
    "Note: in this version, it is assumed that NaN (Not a Number) takes the value of zero, but other interpretations such as -99.9, used in the relevant study, are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19275f",
   "metadata": {},
   "source": [
    "##### ML_implementation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f51f41",
   "metadata": {},
   "source": [
    "In this notebook, the focus was solely on a practical application of predicting physical quantities for the three algorithms xgboost, lightgbm, and catboost, using these parameters, saving them in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f04d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    XGBRegressor(\n",
    "        objective='reg:squarederror', \n",
    "        n_estimators=100,\n",
    "        max_depth=8,\n",
    "        nthread=-1,\n",
    "    ),\n",
    "    LGBMRegressor(\n",
    "        objective='regression', \n",
    "        n_jobs=-1,\n",
    "        n_estimators=100,\n",
    "        max_depth=8,\n",
    "        subsample=0.8,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    CatBoostRegressor(\n",
    "        loss_function='RMSE',   \n",
    "        logging_level='Silent',\n",
    "        n_estimators=100,\n",
    "        max_depth=8\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf653d",
   "metadata": {},
   "source": [
    "Used train-validation-test with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffeabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c7c641",
   "metadata": {},
   "source": [
    "##### ML_algorithm_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755f180",
   "metadata": {},
   "source": [
    "Here, we focus solely on the development of an analysis, opting for the CatBoost algorithm and using MultiOutputRegression, (with LePhare labels) which has proven to be the most efficient for a reasonable time frame. distribution, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa33a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "y = target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "models = [\n",
    "    CatBoostRegressor(\n",
    "        n_estimators = 200,\n",
    "        max_depth = 8,\n",
    "        verbose=0\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24409694",
   "metadata": {},
   "source": [
    "Metrics were established for the predicted labels, and various graphical studies were presented, such as lines separating catastrophic outliers from other points and histograms depicting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_redshift(x,y):\n",
    "    met = np.abs(pd.Series(y-x))\n",
    "    f_out = met/(1+x.astype(np.float32))\n",
    "    nmad=1.48*np.median(f_out)\n",
    "    bias = np.median(f_out)\n",
    "    y_outlier = pd.Series(np.where(f_out > 0.15, 'outlier', 'not outlier'))\n",
    "    print(\"{}\\n\".format(y_outlier.value_counts()))\n",
    "    print(\"Bias: {:.2f}\\n\".format(bias))\n",
    "    print(\"NMAD score: {:.2f}\\n\".format(nmad))\n",
    "    print('r2= {:.2f}'.format(r2_score(x, y)))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(x, y):\n",
    "    f_out = np.abs(pd.Series(y-x))\n",
    "    nmad=1.48 * np.median(f_out)\n",
    "    bias = np.median(f_out)\n",
    "    y_outlier = pd.Series(np.where(f_out > 0.3, 'outlier', 'not outlier'))\n",
    "    print(\"{}\\n\".format(y_outlier.value_counts()))\n",
    "    print(\"Bias: {:.2f}\\n\".format(bias))\n",
    "    print(\"NMAD score: {:.2f}\\n\".format(nmad))\n",
    "    print('r2= {:.2f}'.format(r2_score(x, y)))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5284ef8c",
   "metadata": {},
   "source": [
    "##### ML_algorithm_analysis_EASY.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50206b7",
   "metadata": {},
   "source": [
    "Here we performed the exact same analysis, only using the labels provided by the EASY software."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
